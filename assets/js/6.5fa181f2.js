(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{351:function(t,a,e){"use strict";e.r(a);var s=e(42),n=Object(s.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"table-of-content"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#table-of-content"}},[t._v("#")]),t._v(" Table of Content")]),t._v(" "),e("p"),e("div",{staticClass:"table-of-contents"},[e("ul",[e("li",[e("a",{attrs:{href:"#abbreviations"}},[t._v("Abbreviations")])]),e("li",[e("a",{attrs:{href:"#abstract"}},[t._v("Abstract")])]),e("li",[e("a",{attrs:{href:"#chapter-1-introduction"}},[t._v("Chapter 1: Introduction")])]),e("li",[e("a",{attrs:{href:"#chapter-2-motivation-and-problem-statement"}},[t._v("Chapter 2: Motivation and Problem Statement")]),e("ul",[e("li",[e("a",{attrs:{href:"#_2-1-motivation"}},[t._v("2.1: Motivation")])]),e("li",[e("a",{attrs:{href:"#_2-2-problem-statement"}},[t._v("2.2: Problem Statement")])])])]),e("li",[e("a",{attrs:{href:"#chapter-3-literature-review"}},[t._v("Chapter 3: Literature Review")])]),e("li",[e("a",{attrs:{href:"#chapter-4-materials-and-methods"}},[t._v("Chapter 4: Materials and Methods")]),e("ul",[e("li",[e("a",{attrs:{href:"#_4-1-materials"}},[t._v("4.1: Materials")])]),e("li",[e("a",{attrs:{href:"#_4-2-methods"}},[t._v("4.2: Methods")])])])]),e("li",[e("a",{attrs:{href:"#chapter-5-implementation-details"}},[t._v("Chapter 5: Implementation Details")]),e("ul",[e("li",[e("a",{attrs:{href:"#_5-1-snippets-of-model-training-code"}},[t._v("5.1: Snippets of Model Training Code")])]),e("li",[e("a",{attrs:{href:"#_5-2-snippets-of-app-user-interface"}},[t._v("5.2: Snippets of App User Interface")])])])]),e("li",[e("a",{attrs:{href:"#chapter-6-results"}},[t._v("Chapter 6: Results")])]),e("li",[e("a",{attrs:{href:"#references"}},[t._v("References")])])])]),e("p"),t._v(" "),e("h2",{attrs:{id:"abbreviations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#abbreviations"}},[t._v("#")]),t._v(" Abbreviations")]),t._v(" "),e("p",[e("strong",[t._v("ML")]),t._v(" Machine Learning"),e("br"),t._v(" "),e("strong",[t._v("DL")]),t._v(" Deep Learning"),e("br"),t._v(" "),e("strong",[t._v("CNN")]),t._v(" Convolutional Neural Network"),e("br"),t._v(" "),e("strong",[t._v("DCNN")]),t._v(" Deep Convolutional Neural Network"),e("br"),t._v(" "),e("strong",[t._v("ILSVRC")]),t._v(" ImageNet Large Scale Visual Recognition Challenge"),e("br"),t._v(" "),e("strong",[t._v("VGG")]),t._v(" Visual Geometry Group "),e("strong",[t._v("(")]),t._v(" Convolutional neural network architecture "),e("strong",[t._v(")")]),e("br"),t._v(" "),e("strong",[t._v("COCO")]),t._v(" Common Objects in Context")]),t._v(" "),e("h2",{attrs:{id:"abstract"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#abstract"}},[t._v("#")]),t._v(" Abstract")]),t._v(" "),e("p",[t._v("With the increase in population, the needs have also been increased. As agriculture is a vital source for providing food, whereas crop ailments are major commination to agriculture, and their early detection remains strenuous over the globe due to insufficient technology, agricultural organizations are unable to reach farmers in time for necessary precautionary measures. As a result, farmers have to suffer from compromised lower crop yield. Many machine learning models were used to detect and identify diseases of plants but, after the advancement in Deep Learning, this field seems to have great potential concerning improved accuracy. The combination of advancements in computer vision and the global smartphone penetration made possible by deep learning to provide a mobile-phone-based system to diagnose diseases. Using a public data-set of "),e("strong",[t._v("87,900")]),t._v(" photos of healthy and diseased leaves, several models were trained to identify "),e("strong",[t._v("14")]),t._v(" crops and the presence or absence of "),e("strong",[t._v("26")]),t._v(" diseases, with the best performance reaching a "),e("strong",[t._v("98.58%")]),t._v(" on the retained dataset, which demonstrates the practicality of our perspective. Generally, the methodology of training deep convolutional neural networks on exceptionally huge and publicly accessible image data-sets presents a straightforward path towards a massive global diagnosis of mobile-phone-assisted crop disease.")]),t._v(" "),e("h2",{attrs:{id:"chapter-1-introduction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#chapter-1-introduction"}},[t._v("#")]),t._v(" Chapter 1: Introduction")]),t._v(" "),e("p",[t._v("Earth is occupying more than 7 billion people. The number is increasing gradually.\nWith a persistent increase in population, it is understood that basic life necessities\nare also increasing in which food is on top of the list. The increasing population\nhas also eaten up the land, and the area for agricultural land is shrinking up.\nSo we have to get an adequate amount of food from the land available. Food\nsustainability, however, remains endanger by a variety of factors which includes\nclimate change ["),e("a",{attrs:{href:"#references"}},[t._v("5")]),t._v("], plant diseases ["),e("a",{attrs:{href:"#references"}},[t._v("23")]),t._v("] and others. When the plants are growing up,\nthey are attacked by diseases which clearly means a compromised lower crop yield.\nUnfortunately, smallholder farmers have to face the disastrous consequences whose\nincome entirely depends upon crops. Yield. The major agricultural production\ncomes from smallholder farmers ["),e("a",{attrs:{href:"#references"}},[t._v("9")]),t._v("], and they have to suffer approximately 50%\nyield loss due to climate change, pest attack, and diseases. Acknowledging these\nproblems, various attempts have been made to avert or lower the loss of crop yield.\nTo prevent the disease, it is crucial to detect the disease at an early stage. And\nefficient disease management is a very crucial step in this regard. Agricultural\norganizations have been working for disease detection at an early stage at local\nclinics. During the past decades the world has totally turned into ”Global Village”\nand because of that enormous data is available online including information on\ndisease diagnosis ["),e("a",{attrs:{href:"#references"}},[t._v("11")]),t._v("] and the leverage of which is internet penetration worldwide.\nMore recently, mobile phone technology incredibly has become famous due to the\nproliferation of mobile-based tools in all parts of the world. All these factors\nescort us to a point where disease detection is technically feasible and available at\nan unparalleled scale. Unlike other countries, Pakistan lacks modern technology\ndue to which, we are unable to detect diseases in time and to reach farmers in\norder to raise awareness about rehabilitation. The blue-collar approaches make\nit very slow for policy-making organizations to gather data and draw results for\nimmediate movement. The intensity behind this research work is to dispense\nan efficient system that can detect the disease straight-away whether a farmer\nor agricultural organization uses it. We intended to develop mobile as well as\na web-based tool using deep learning for the detection of crop diseases. Deep\nlearning has proved its worth successfully in many different domains such as\nend-to-end learning. Now we are going to demonstrate the technical feasibility of our\nproposed approach by utilizing 87,900 images on healthy and infected leaves of\ncrop plants that are openly available on the online system PlantVillage ["),e("a",{attrs:{href:"#references"}},[t._v("2")]),t._v("].")]),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/ExampleDatasetImages.jpg"),alt:"Sample leaf images from the dataset"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 1.1:")]),t._v(" Sample leaf images from the dataset")]),t._v(" "),e("hr"),t._v(" "),e("p",[t._v("A DCNN includes the mapping between an input to an output. Deep learning\nis probability-based means it never gives us the definite answer however it gives\nus the probabilities. The term CNN itself stipulates that a mathematical function called convolution is used within the network. Convolution is a specific type\nmathematical operation on two functions which produces a third function that\nexpresses how another changes one’s form. A CNN consists of two main input and\noutput layers, and multiple hidden layers. Typically the hidden layers consist of\na series of convolutionary layers. The activation function is commonly a Rectified\nLinear Units (RELU). The purpose of RELU is to normalize the values after the\napplication of convolution to convert the values in a specific range. Additional\nconvolutions such as pooling, fully connected layers and normalization layers follow the RELU. Pooling is to choose the best or one thing out of the pool of things.\nThe nodes in neural networks are computational units which take weighted inputs\nfrom the incoming edges and provide an outgoing edge with numerical output.\nnode enumerates an output value by adding a specific function to the previous\nlayer’s input values. In a neural network, model learning progresses through iterative changes to these weights and biases. DCNN is learned by changing network\nparameters in such a way as to improve mapping during the training. For thepurpose of plant disease identification, we needed a large and verified data-set of\nhealthy and diseased images to train an accurate image classifier, but such dataset\ndid not exist until very recently, and even small dataset were not publicly available. In order to tackle this issue, the PlantVillage project began to collect a large\ndataset of diseased and healthy crop plants and made them available publicly. We\nannounce here on the classification of 26 diseases (presence or absence) in 14 crop\nspecies using 87,900 with deep learning (DL).")]),t._v(" "),e("h2",{attrs:{id:"chapter-2-motivation-and-problem-statement"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#chapter-2-motivation-and-problem-statement"}},[t._v("#")]),t._v(" Chapter 2: Motivation and Problem Statement")]),t._v(" "),e("h3",{attrs:{id:"_2-1-motivation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-motivation"}},[t._v("#")]),t._v(" 2.1: Motivation")]),t._v(" "),e("p",[t._v("With the increase in population, the needs have also been increased. As agriculture is the vital source for providing food but, unfortunately, because of lack the\nmodern technology, the agricultural organization are unable to reach farmers in\ntime for necessary precautionary measures. As a results farmers have to suffer\nfrom compromised lower crop yield. We are aimed to work for a solution that can\nprevent farmers from suffering a loss to some extent.")]),t._v(" "),e("h3",{attrs:{id:"_2-2-problem-statement"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-problem-statement"}},[t._v("#")]),t._v(" 2.2: Problem Statement")]),t._v(" "),e("p",[t._v("Plant disease affects not only the production of human food but also natural\nsystems. The majority of smallholder farmers do not have access to the resources\nthat can identify the diseases accurately and timely. Due to the lack of timely and\naccurate agricultural information, they have to suffer from lower crop yield.")]),t._v(" "),e("h2",{attrs:{id:"chapter-3-literature-review"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#chapter-3-literature-review"}},[t._v("#")]),t._v(" Chapter 3: Literature Review")]),t._v(" "),e("p",[t._v("The Deep Learning (DL) is subcategory of ML. This field is still evolving. Machine\nLearning has made enormous evolution in the past few years. Many advances were\nfound in the first phase, such as handwritten text recognition, back-propagation\nand resolving training problems. In the second phase was to develop algorithms\nfor health-sectors, text-recognition, earthquake-predictions, marketing, finance,\nimage-recognition, and object detection. In 2012 a deep convolutionary neural\nnetwork accomplished a top-5 error of 15.3% when classifying images into 1000\npossible categories ["),e("a",{attrs:{href:"#references"}},[t._v("1")]),t._v("]. In the next three years, numerous advances in convolutionary neural networks reduced the error rate to 3.57. With the passage of time as\nDeep Learning architectures started to evolve, researchers applied them to classification, segmentation, object detection, video processing, natural language processing, image recognition, and speech recognition. Different agriculture application\nhas also been developed using these architectures. For example, Leaf counting was\nperformed using Deep CNN with average accuracy of 95% ["),e("a",{attrs:{href:"#references"}},[t._v("25")]),t._v("]. Leaf classification\nwas performed using deep convolutional neural network classifier among 32 different species with average accuracy of 97.3% ["),e("a",{attrs:{href:"#references"}},[t._v("7")]),t._v("]. Fruit counting was performed by\nusing simulated deep convolutional neural network with an average accuracy of\n91% ["),e("a",{attrs:{href:"#references"}},[t._v("19")]),t._v("]. Classification of land cover and crop type was performed by using deep\nlearning classifier with accuracy of crop type identification of 95% ["),e("a",{attrs:{href:"#references"}},[t._v("18")]),t._v("]. Identification of plants was performed by using Deep convolutional neural network ["),e("a",{attrs:{href:"#references"}},[t._v("10")]),t._v("], ["),e("a",{attrs:{href:"#references"}},[t._v("16")]),t._v("].\nIn ["),e("a",{attrs:{href:"#references"}},[t._v("26")]),t._v("] identification of plants was performed among 100 different species utilizing\n10,000 images using deep learning with an accuracy of 91.78%. In addition deep\nlearning techniques are also used in for crucial tasks such as crop plant disease\nidentification and classification which is main topic of this thesis. For example,\nin ["),e("a",{attrs:{href:"#references"}},[t._v("20")]),t._v("] plant disease detection was performed using deep convolutional neural network classifier. To sum up, used DL Architectures are shown in the table along with the selected plants and their results.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Deep Learning Techniques")]),t._v(" "),e("th",[t._v("Dataset")]),t._v(" "),e("th",[t._v("Used Plants")]),t._v(" "),e("th",[t._v("Accuracy")]),t._v(" "),e("th",[t._v("Reference")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Convolutional Neural Network")]),t._v(" "),e("td",[t._v("Plant Village ")]),t._v(" "),e("td",[t._v("Maize")]),t._v(" "),e("td",[t._v("92.85%")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("21")]),t._v("]")])]),t._v(" "),e("tr",[e("td",[t._v("LeNet")]),t._v(" "),e("td",[t._v("Plant Village ")]),t._v(" "),e("td",[t._v("Banana")]),t._v(" "),e("td",[t._v("98.54%")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("4")]),t._v("]")])]),t._v(" "),e("tr",[e("td",[t._v("Alex-Net, VGG16, VGG-19, Squeeze-Net, Goog-LeNet, Inception-v-3, Inception-ResNet-v-2, ResNet-50, Resnet-101 ")]),t._v(" "),e("td",[t._v("Real Field Dataset")]),t._v(" "),e("td",[t._v("Apricot, Walnut, Peach, Cherry ")]),t._v(" "),e("td",[t._v("97.14%")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("24")]),t._v("]")])]),t._v(" "),e("tr",[e("td",[t._v("Inception-v-3 ")]),t._v(" "),e("td",[t._v("Experimental Field Dataset ")]),t._v(" "),e("td",[t._v("Cassava ")]),t._v(" "),e("td",[t._v("93%")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("3")]),t._v("]")])]),t._v(" "),e("tr",[e("td",[t._v("Convolutional Neural Network")]),t._v(" "),e("td",[t._v("Images Taken From The Research Center ")]),t._v(" "),e("td",[t._v("Cucumber ")]),t._v(" "),e("td",[t._v("82.3%")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("8")]),t._v("]")])]),t._v(" "),e("tr",[e("td",[t._v("Super Resolution Convolutional Neural Network (SCRNN)")]),t._v(" "),e("td",[t._v("Plant Village ")]),t._v(" "),e("td",[t._v("Tomato ")]),t._v(" "),e("td",[t._v("90%")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("15")]),t._v("]")])]),t._v(" "),e("tr",[e("td",[t._v("Caffe-Net")]),t._v(" "),e("td",[t._v("Downloaded From The Internet ")]),t._v(" "),e("td",[t._v("Pear, cherry, peach, apple, grapevine ")]),t._v(" "),e("td",[t._v("96.3%")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("22")]),t._v("]")])]),t._v(" "),e("tr",[e("td",[t._v("Resnet-50, Inception-V-2, Mobile-Net-V-1 ")]),t._v(" "),e("td",[t._v("Real Environment ")]),t._v(" "),e("td",[t._v("Banana ")]),t._v(" "),e("td",[t._v("99% of ResNet-50")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("17")]),t._v("]")])]),t._v(" "),e("tr",[e("td",[t._v("Mobile-Net, Modified-Mobile-Net, Reduced-Mobile-Net")]),t._v(" "),e("td",[t._v("Plant Village dataset")]),t._v(" "),e("td",[t._v("24 Types Of Plants")]),t._v(" "),e("td",[t._v("98.34% of reduced MobileNet ")]),t._v(" "),e("td",[t._v("["),e("a",{attrs:{href:"#references"}},[t._v("14")]),t._v("]")])])])]),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Table 3.1:")]),t._v(" DL Architectures along with selected plant species and results")]),t._v(" "),e("hr"),t._v(" "),e("p",[t._v("From this table, we can conclude that while some Deep Learning Architectures/-\nModels have been developed for the identification of the diseases of plants but this\nis still a prolific research field and should lead to improvements for better plant\ndisease identification. So we needed a large and verified data set of healthy and\ndiseased images to train an accurate image classifier for plant disease diagnosis,\nbut such a dataset did not exist until very recently. Plant Village collected a huge\ndataset of 87,900 images of plant health to enable the development of smartphone\nassisted disease diagnosis and made it publicly and freely available ["),e("a",{attrs:{href:"#references"}},[t._v("12")]),t._v("].")]),t._v(" "),e("h2",{attrs:{id:"chapter-4-materials-and-methods"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#chapter-4-materials-and-methods"}},[t._v("#")]),t._v(" Chapter 4: Materials and Methods")]),t._v(" "),e("h3",{attrs:{id:"_4-1-materials"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-materials"}},[t._v("#")]),t._v(" 4.1: Materials")]),t._v(" "),e("p",[t._v("Our dataset is consists of 87,900 pictures of healthy and diseased plant leaves. All\nthe images in the dataset were taken at experimental research stations affiliated\nwith Land Grant Universities in the USA (Penn State, Florida State, Cornell, and\nothers) ["),e("a",{attrs:{href:"#references"}},[t._v("12")]),t._v("]. These images consist of 26 major crop diseases (4 bacterial diseases,\n17 Fungal diseases, 2 viral diseases, 2 mold diseases, and 1 disease caused by a\nmite). These images also have 12 healthy leaves of 12 crop species that are not\nvisibly affected by a disease. Table 4.1 Summarizes the dataset.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Crop")]),t._v(" "),e("th",[t._v("Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Apple")]),t._v(" "),e("td",[t._v("Apple-Healthy, Apple-Cedar-Rust, Apple-Black-Rot, Apple-Scab")])]),t._v(" "),e("tr",[e("td",[t._v("Raspberry")]),t._v(" "),e("td",[t._v("Raspberry-Healthy")])]),t._v(" "),e("tr",[e("td",[t._v("Soybean")]),t._v(" "),e("td",[t._v("Soybean-Healthy")])]),t._v(" "),e("tr",[e("td",[t._v("Squash")]),t._v(" "),e("td",[t._v("Squash-Powdery-Mildew")])]),t._v(" "),e("tr",[e("td",[t._v("Strawberry")]),t._v(" "),e("td",[t._v("Strawberry-Leaf-Scorch, Strawberry-Healthy")])]),t._v(" "),e("tr",[e("td",[t._v("Tomato")]),t._v(" "),e("td",[t._v("\n        Tomato-Healthy, Tomato-Late-Blight, TomatoBacterial-Spot,\n        Tomato-Early-Blight, TomatoLeaf-Mold, Tomato-Septoria-Leaf-Spot,\n        TomatoMosaic-Virus, Tomato-Two-Spotted-Spider-Mite, Tomato-Target-Spot,\n        Tomato-Yellow-Leaf-CurlVirus\n      ")])]),t._v(" "),e("tr",[e("td",[t._v("Potato")]),t._v(" "),e("td",[t._v("Potato-Early-Blight, Potato-Late-Blight, PotatoHealthy")])]),t._v(" "),e("tr",[e("td",[t._v("Blueberry")]),t._v(" "),e("td",[t._v("Blueberry-Healthy")])]),t._v(" "),e("tr",[e("td",[t._v("Cherry")]),t._v(" "),e("td",[t._v("Cherry-Healthy, Cherry-Powdery-Mildew")])]),t._v(" "),e("tr",[e("td",[t._v("Corn")]),t._v(" "),e("td",[t._v("\n        Corn-Healthy, Corn-Common-Rust, Corn-Gray-Leaf Spot,\n        Corn-Northern-Leaf-Blight\n      ")])]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("Grape")]),t._v(" "),e("td",[t._v("\n        Grape-Healthy, Grape-Leaf-Blight, Grape-BlackRot,\n        Grape-Black-Measles(Esca)\n      ")])]),t._v(" "),e("tr",[e("td",[t._v("Orange")]),t._v(" "),e("td",[t._v("Orange-Huanglongbing (Citrus-Greening)")])]),t._v(" "),e("tr",[e("td",[t._v("Peach")]),t._v(" "),e("td",[t._v("Peach-Healthy, Peach-Bacterial-Spot")])]),t._v(" "),e("tr",[e("td",[t._v("Bell-Pepper")]),t._v(" "),e("td",[t._v("Bell-Pepper-Bacterial-Spot, Bell-Pepper-Healthy")])])])]),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Table 4.1:")]),t._v(" List of Crops with their respective classes in the Dataset.")]),t._v(" "),e("hr"),t._v(" "),e("h3",{attrs:{id:"_4-2-methods"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-methods"}},[t._v("#")]),t._v(" 4.2: Methods")]),t._v(" "),e("p",[t._v("Since these diseases can severely affect plants, we landed up with four different\nmodels for 38 different classes (Table 4.1) to achieve maximum accuracy. Blueprint\nof our proposed system for disease detection is showed in the figure 4.1.")]),t._v(" "),e("p",[t._v("To train the model, we focused on 4 different architectures, namely VGG16,\nResnet152, MobileNet, and MobileNetV2.")]),t._v(" "),e("p",[t._v("VGG16 was proposed by Andrew Zisserman and Karen Simonyan of the Visual\nGeometry Community Lab at Oxford University. This model secured 1st and 2nd\nplace in ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014.\nThe VGG16 network is trained on an ImageNet dataset which has 14 million\nimages and 1000 classes, and this model achieves 92.7% top-5 test accuracy.")]),t._v(" "),e("p",[t._v("Resnet was proposed by Microsoft Research Asia ["),e("a",{attrs:{href:"#references"}},[t._v("13")]),t._v("] and won the ImageNet\nLarge Scale Visual Recognition Challenge (ILSVRC) and MS-COCO competition in 2015. ResNet architecture has several variants all of them have the similar\nconcept but have different number of layers, for example ResNet-18, ResNet-50,\nResNet-101 and so on. The name Resnet followed by numbers simply insinuate\nthe Resnet architecture witch has number of layers of neural network. The main\nidea manoeuvred in these models, residual network connections, is found to greatly\nenhance gradient flow, thus enabling the training of tens or even hundreds of layers\nof much deeper models.")]),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/fyp_model_image.jpeg"),alt:"Blueprint of our proposed system for disease detection"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 4.1:")]),t._v(" Blueprint of our proposed system for disease detection")]),t._v(" "),e("hr"),t._v(" "),e("p",[t._v("MobileNet is a model built primarily from depthwise separable convolutions to\ncreate lightweight deep convolutional neural networks and provides an efficient\nmodel for smartphone and integrated vision applications ["),e("a",{attrs:{href:"#references"}},[t._v("6")]),t._v("].")]),t._v(" "),e("p",[t._v("To train these models we used Google Colaboratory framework. Google Colab\nis a free cloud service created by Google, in which the person who has a Gmail\naccount can write, execute codes for Machine learning as well as for Deep learning.\nIn Google Colab different runtime environments and Various versions of python\nare available. It can also download large datasets to google drive at higher speed\ndirectly from the servers. With Google Colab, you can also mount your drive and it\ncan fetch the appropriate file after the authentication. The most important feature\nof Google Colab that distinguishes it from other free cloud services is it provides\nGPU and it is totally free. To summarize, we have 4 experimental configurations\nin total, which are described in the next section")]),t._v(" "),e("h4",{attrs:{id:"_4-2-1-experiment-1"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-1-experiment-1"}},[t._v("#")]),t._v(" 4.2.1: Experiment # 1")]),t._v(" "),e("p",[t._v("In this experiment, we used a python deep learning library called PyTorch, and\nthe model we used in this experiment is Resnet152. We used transfer learning to\ntrain the model because it always yields better results. This experiment runs for\na total of 50 epochs and achieves testing accuracy of 98.5%.")]),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/fyp-model-accuracy-pytorch.png"),alt:"Model Accuracy"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 4.2:")]),t._v(" Model Accuracy")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/fyp-model-loss-pytorch.png"),alt:"Model Loss"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 4.3:")]),t._v(" Model Loss")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_4-2-2-experiment-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-2-experiment-2"}},[t._v("#")]),t._v(" 4.2.2: Experiment # 2")]),t._v(" "),e("p",[t._v("In this experiment, we used a python deep learning library called TensorFlow, and\nwe created a custom model. This experiment runs for a total of 80 epochs and\nachieves an accuracy of 94.72%.")]),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/precision-recall-accuracy-f1-score-custome-model.png"),alt:"Accuracy of the classifier for all classess"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 4.4:")]),t._v(" Accuracy of the classifier for all classes")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/classifier-graph.png"),alt:"Graph of all Classes and their Identifications"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 4.4:")]),t._v(" Graph of all Classes and their Identifications")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"chapter-5-implementation-details"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#chapter-5-implementation-details"}},[t._v("#")]),t._v(" Chapter 5: Implementation Details")]),t._v(" "),e("p",[t._v("The very first was to gather the data and train the model to get the results. After\nthe collection of the dataset which is consisted of 87,900 different plant images, we\ndivided the dataset into train-test splits of 25-75, 50-50, and 25-75. We made these\nsplits to get a sense of how our proposed approach will perform on the unseen data\nand also keep track of if our proposed approach is underfitting or overfitting. After\ntraining the model with the best performance reaching a 98.58% on a held-out test\nset we developed an API using Flask for this model to interact with our mobile\napplication. After the development of the API, we developed a mobile application\nto interact with the API which interacts with the model to give the final results.")]),t._v(" "),e("h3",{attrs:{id:"_5-1-snippets-of-model-training-code"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-snippets-of-model-training-code"}},[t._v("#")]),t._v(" 5.1: Snippets of Model Training Code")]),t._v(" "),e("p",[t._v("Here are some snippets of our code to train the model.")]),t._v(" "),e("div",{staticClass:"language-py extra-class"},[e("pre",{pre:!0,attrs:{class:"language-py"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" time\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optim\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functional "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" F\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torchvision "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" datasets"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" models\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torchvision\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" collections "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" OrderedDict\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("autograd "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Variable\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" PIL "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Image\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" lr_scheduler\n")])])]),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.1:")]),t._v(" Libraries used in the model training")]),t._v(" "),e("hr"),t._v(" "),e("div",{staticClass:"language-py extra-class"},[e("pre",{pre:!0,attrs:{class:"language-py"}},[e("code",[t._v("data_dir "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/content/DataSet'")]),t._v("\ntrain_dir "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_dir "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/train'")]),t._v("\nvalid_dir "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_dir "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/val'")]),t._v("\nnThreads "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\nbatch_size "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),t._v("\n")])])]),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.2:")]),t._v(" Load the dataset")]),t._v(" "),e("hr"),t._v(" "),e("div",{staticClass:"language-py extra-class"},[e("pre",{pre:!0,attrs:{class:"language-py"}},[e("code",[t._v("data_transforms "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Compose"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RandomRotation"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RandomResizedCrop"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RandomHorizontalFlip"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ToTensor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Normalize"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.485")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.456")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.406")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.229")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.224")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.225")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Compose"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Resize"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CenterCrop"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ToTensor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Normalize"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.485")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.456")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.406")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.229")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.224")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.225")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\ndata_dir "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/content/DataSet'")]),t._v("\nimage_datasets "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" datasets"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ImageFolder"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_dir"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                          data_transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" x "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\ndataloaders "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataLoader"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_datasets"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_size"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_size"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                             shuffle"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_workers"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n              "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" x "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\ndataset_sizes "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_datasets"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" x "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\nclass_names "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" image_datasets"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes\n")])])]),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.3:")]),t._v(" Applying Data Augmentation")]),t._v(" "),e("hr"),t._v(" "),e("div",{staticClass:"language-py extra-class"},[e("pre",{pre:!0,attrs:{class:"language-py"}},[e("code",[t._v("model "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" models"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resnet152"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pretrained"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" param "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    param"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("requires_grad "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" collections "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" OrderedDict\n\nclassifier "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("OrderedDict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n                          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fc1'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2048")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ReLU"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fc2'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("39")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'output'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("LogSoftmax"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dim"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" classifier\n")])])]),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.4:")]),t._v(" Building the Classifier")]),t._v(" "),e("hr"),t._v(" "),e("div",{staticClass:"language-py extra-class"},[e("pre",{pre:!0,attrs:{class:"language-py"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("train_model")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" criterion"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimizer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" scheduler"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_epochs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    since "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("time"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    best_model_wts "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" copy"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deepcopy"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    best_acc "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" epoch "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_epochs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Epoch {}/{}'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epoch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_epochs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'-'")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" phase "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" phase "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                scheduler"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n            "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("eval")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n\n            running_loss "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),t._v("\n            running_corrects "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n\n            "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" inputs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" dataloaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("phase"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                inputs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" inputs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                optimizer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n                "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_grad_enabled"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("phase "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    outputs "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    loss "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" criterion"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("outputs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    _"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" preds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("outputs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n                    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" phase "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        loss"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                        optimizer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n                running_loss "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" loss"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("item"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" inputs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                running_corrects "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            epoch_loss "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" running_loss "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" dataset_sizes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("phase"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            epoch_acc "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" running_corrects"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("double"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" dataset_sizes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("phase"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{} Loss: {:.4f} Acc: {:.4f}'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                phase"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epoch_loss"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epoch_acc"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" phase "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" epoch_acc "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" best_acc"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                best_acc "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" epoch_acc\n                best_model_wts "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" copy"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deepcopy"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    time_elapsed "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("time"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" since\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Training complete in {:.0f}m {:.0f}s'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        time_elapsed "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("60")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" time_elapsed "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("60")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Best valid accuracy: {:4f}'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("best_acc"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_state_dict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("best_model_wts"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" model\n")])])]),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.5:")]),t._v(" Function to train the model")]),t._v(" "),e("hr"),t._v(" "),e("div",{staticClass:"language-py extra-class"},[e("pre",{pre:!0,attrs:{class:"language-py"}},[e("code",[t._v("num_epochs "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("80")]),t._v("\n\ncriterion "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("NLLLoss"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noptimizer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" optim"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nexp_lr_scheduler "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lr_scheduler"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("StepLR"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" step_size"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gamma"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel_ft "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" criterion"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimizer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" exp_lr_scheduler"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_epochs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("num_epochs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.6:")]),t._v(" : Start Training")]),t._v(" "),e("hr"),t._v(" "),e("div",{staticClass:"language-py extra-class"},[e("pre",{pre:!0,attrs:{class:"language-py"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("test")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataloaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" device"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("eval")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  accuracy "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" images"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" dataloaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    images "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Variable"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    labels "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Variable"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    images"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" images"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    output "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    ps "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exp"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    equality "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" ps"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    accuracy "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" equality"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type_as"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("FloatTensor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Testing Accuracy: {:.3f}"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("accuracy"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dataloaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntest"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataloaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" device"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.7:")]),t._v(" Model validation after Training")]),t._v(" "),e("hr"),t._v(" "),e("div",{staticClass:"language-py extra-class"},[e("pre",{pre:!0,attrs:{class:"language-py"}},[e("code",[t._v("model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("class_to_idx "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataloaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("class_to_idx\nmodel"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("epochs "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_epochs\ncheckpoint "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_size'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'batch_size'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataloaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("batch_size"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'output_size'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("39")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'state_dict'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data_transforms'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" data_transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'optimizer_dict'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("optimizer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'class_to_idx'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("class_to_idx"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'epoch'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" model"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("epochs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\ntorch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("checkpoint"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'plants9615_checkpoint.pth'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.8:")]),t._v(" Saving the model for Further use")]),t._v(" "),e("hr"),t._v(" "),e("h3",{attrs:{id:"_5-2-snippets-of-app-user-interface"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-snippets-of-app-user-interface"}},[t._v("#")]),t._v(" 5.2: Snippets of App User Interface")]),t._v(" "),e("p",[t._v("User interface for the mobile application is shown in the upcoming pictures.")]),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/2_sign_up_page.png"),alt:"Sign Up Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.9:")]),t._v(" Sign Up Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/1_login_page.png"),alt:"Login Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.10:")]),t._v(" Login Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/3_home_page.png"),alt:"Home Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.11:")]),t._v(" Home Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/4_community_page.png"),alt:"Community Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.12:")]),t._v(" Community Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/5_user_page.png"),alt:"User Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.13:")]),t._v("User Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/6_add_post_page.png"),alt:"Add Post Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.14:")]),t._v("Add Post Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/7_logout_nav.png"),alt:"Logout Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.15:")]),t._v(" Logout Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/8_prediction1.png"),alt:"Prediction # 1 Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.16:")]),t._v(" Prediction # 1 Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/9_prediction2.png"),alt:"Prediction # 2 Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.17:")]),t._v(" Prediction # 2 Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/10_prediction3.png"),alt:"Prediction # 3 Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.18:")]),t._v(" Prediction # 3 Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/11_prediction4.png"),alt:"Prediction # 4 Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.19:")]),t._v(" Prediction # 4 Screen")]),t._v(" "),e("hr"),t._v(" "),e("img",{attrs:{src:t.$withBase("/thesis/12_prediction5.png"),alt:"Prediction # 5 Screen",width:"300",height:"568"}}),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Figure 5.20:")]),t._v(" Prediction # 5 Screen")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"chapter-6-results"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#chapter-6-results"}},[t._v("#")]),t._v(" Chapter 6: Results")]),t._v(" "),e("p",[t._v("All results were observed on the premise that the trained model has to identify\nthe crop genus and the state of the infection. After training model with different\nconfigurations the overall accuracy we procured on the collected dataset varied\nfrom 75% to 98.58% which showing us the strong promise of a DL approach to\nsimilar problems. Mean accuracy’s across our experiments is shown in the table\ngiven below.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Experiment")]),t._v(" "),e("th",[t._v("Category")]),t._v(" "),e("th",[t._v("Dataset Splits")]),t._v(" "),e("th",[t._v("Model Used")]),t._v(" "),e("th",[t._v("Epochs")]),t._v(" "),e("th",[t._v("Mean Accuracy")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("1")]),t._v(" "),e("td",[t._v("Transfer Learning")]),t._v(" "),e("td",[t._v("70% Train, 30% Validation, Testing")]),t._v(" "),e("td",[t._v("VGG16")]),t._v(" "),e("td",[t._v("30")]),t._v(" "),e("td",[t._v("94.72%")])]),t._v(" "),e("tr",[e("td",[t._v("2")]),t._v(" "),e("td",[t._v("Transfer Learning")]),t._v(" "),e("td",[t._v("70% Train, 30% Validation, Testing")]),t._v(" "),e("td",[t._v("Resnet152")]),t._v(" "),e("td",[t._v("30")]),t._v(" "),e("td",[t._v("96.43%")])]),t._v(" "),e("tr",[e("td",[t._v("3")]),t._v(" "),e("td",[t._v("Transfer Learning")]),t._v(" "),e("td",[t._v("70% Train, 30% Validation, Testing")]),t._v(" "),e("td",[t._v("Resnet152")]),t._v(" "),e("td",[t._v("50")]),t._v(" "),e("td",[t._v("98.5849%")])]),t._v(" "),e("tr",[e("td",[t._v("4")]),t._v(" "),e("td",[t._v("Custom Model")]),t._v(" "),e("td",[t._v("70% Train, 30% Validation, Testing")]),t._v(" "),e("td",[t._v("Created Custom Model")]),t._v(" "),e("td",[t._v("50")]),t._v(" "),e("td",[t._v("96%")])])])]),t._v(" "),e("hr"),t._v(" "),e("span",{staticStyle:{"text-align":"center",display:"block"}},[e("strong",[t._v("Table 6.1:")]),t._v(" Mean accuracy’s across all experiments")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"references"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#references"}},[t._v("#")]),t._v(" References")]),t._v(" "),e("p",[e("strong",[t._v("[1]")]),t._v("  Ilya Sutskever Alex Krizhevsky and Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. Online Available at: "),e("a",{attrs:{href:"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("strong",[t._v("[2]")]),t._v("  Abdallah Ali. PlantVillage Dataset Dataset of diseased plant leaf images and corresponding labels. Available at: "),e("a",{attrs:{href:"https://www.kaggle.com/xabdallahali/plantvillage-dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.kaggle.com/xabdallahali/plantvillage-dataset"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("strong",[t._v("[3]")]),t._v(" Peter McCloskey Babuali Ahmed Ally James Legg Amanda Ramcharan,\nKelsee Baranowski and David P. Hughes. Deep Learning for Image-Based\nCassava Disease Detection. Available at: . "),e("a",{attrs:{href:"https://www.frontiersin.org/articles/10.3389/fpls.2017.01852/full?report=reader",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.frontiersin.org/articles/10.3389/fpls.2017.01852/full?report=reader"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[4]")]),t._v(" Bouaziz Bassem Amara Jihen and Algergawy Alsayed. A Deep Learningbased Approach for Banana Leaf Diseases Classification. Available at: .\n"),e("a",{attrs:{href:"https://dl.gi.de/handle/20.500.12116/944",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://dl.gi.de/handle/20.500.12116/944"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[5]")]),t._v(" Maria Val Martin Amos P. K. Tai and Colette L. Heald. Threat to future\nglobal food security from climate change and ozone air pollution. Available\nonline at. "),e("a",{attrs:{href:"https://www.nature.com/articles/nclimate2317",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.nature.com/articles/nclimate2317"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[6]")]),t._v(" Bo Chen Dmitry Kalenichenko Weijun Wang Tobias Weyand Marco Andreetto Andrew G. Howard, Menglong Zhu and Hartwig Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. Available at: . "),e("a",{attrs:{href:"https://arxiv.org/pdf/1704.04861.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://arxiv.org/pdf/1704.04861.pdf"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[7]")]),t._v(" Feras Dayoub Niko Sunderhauf David Hall, Chris McCool and Ben Upcroft. Evaluation of Features for Leaf Classification in Challenging Conditions. Available at: . "),e("a",{attrs:{href:"https://ieeexplore.ieee.org/abstract/document/7045965",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://ieeexplore.ieee.org/abstract/document/7045965"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[8]")]),t._v(" Hiroyuki Uga Satoshi Kagiwada Erika Fujita, Yusuke Kawasaki and Hitoshi Iyatomi. Basic Investigation on a Robust and Practical Plant Diagnostic System. Available at: . "),e("a",{attrs:{href:"https://ieeexplore.ieee.org/abstract/document/7838282",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://ieeexplore.ieee.org/abstract/document/7838282"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[9]")]),t._v(" International Fund for Agricultural Development (IFAD) andUnited Nations Environment Programme (UNEP). Smallholders, food security, and the environment. Available online at. "),e("a",{attrs:{href:"https://www.ifad.org/documents/38714170/39135645/smallholders_report.pdf/133e8903-0204-4e7d-a780-bca847933f2e",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.ifad.org/documents/38714170/39135645/smallholders_report.pdf/133e8903-0204-4e7d-a780-bca847933f2e"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[10]")]),t._v(" Monica G.Larese Guillermo L.Grinblat, Lucas C.Uzal and Pablo M.Granitto. Deep learning for plant identification using vein morphological patterns. Available at: . "),e("a",{attrs:{href:"https://www.sciencedirect.com/science/article/pii/S0168169916304665",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.sciencedirect.com/science/article/pii/S0168169916304665"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[11]")]),t._v(" David. P. Hughes and Marcel Salathe. An open access repository of images on plant health to enable the development of mobile disease diagnostics. Available online at. "),e("a",{attrs:{href:"https://arxiv.org/abs/1511.08060",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://arxiv.org/abs/1511.08060"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("strong",[t._v("[12]")]),t._v(" David P. Hughes and Marcel Salathe. An open access repository of images on plant health to enable the development of mobile disease diagnostics. Available at: . "),e("a",{attrs:{href:"https://arxiv.org/ftp/arxiv/papers/1511/1511.08060.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://arxiv.org/ftp/arxiv/papers/1511/1511.08060.pdf"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("strong",[t._v("[13]")]),t._v(" Shaoqing Ren Jian Sun (Microsoft Research) Kaiming He, Xiangyu Zhang. Deep Residual Learning for Image Recognition. Available at: ."),e("a",{attrs:{href:"https://arxiv.org/pdf/1512.03385.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://arxiv.org/pdf/1512.03385.pdf"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[14]")]),t._v(" Mingyang Wu Kamal KC, Zhendong Yin and Zhilo Wu. Depthwise separable convolution architectures for plant disease classification.Available at: . "),e("a",{attrs:{href:"https://www.sciencedirect.com/science/article/pii/S0168169918318696",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.sciencedirect.com/science/article/pii/S0168169918318696"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[15]")]),t._v(" Takashi Togami Kyosuke Yamamoto and Norio Yamaguchi. Super-Resolution of Plant Disease Images for the Acceleration of Image-based Phenotyping and Vigor Diagnosis in Agriculture. Available at: . "),e("a",{attrs:{href:"https://www.mdpi.com/1424-8220/17/11/2557",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.mdpi.com/1424-8220/17/11/2557"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[16]")]),t._v(" Sue Han Lee and Chee Seng Chan. Deep-plant: Plant identification with convolutional neural networks. Available at: .\n"),e("a",{attrs:{href:"https://ieeexplore.ieee.org/abstract/document/7350839",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://ieeexplore.ieee.org/abstract/document/7350839"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[17]")]),t._v(" Henry Ruiz Nancy Safari Sivalingam Elayabalan Walter Ocimati Michael\nGomez Selvaraj, Alejandro Vergara and Guy Blomme. AI-powered banana\ndiseases and pest detection. Available at: . "),e("a",{attrs:{href:"https://link.springer.com/article/10.1186/s13007-019-0475-z",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://link.springer.com/article/10.1186/s13007-019-0475-z"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[18]")]),t._v(" S. Skakun Nataliia Kussul, Mykola Lavreniuk and Andrii Shelestov. Deep\nLearning Classification of Land Cover and Crop Types Using Remote Sensing\nData. Available at: . "),e("a",{attrs:{href:"https://ieeexplore.ieee.org/document/7891032",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://ieeexplore.ieee.org/document/7891032"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[19]")]),t._v(" Maryam Rahnemoonfar and Clay Sheppard. Deep Count: Fruit Counting\nBased on Deep Simulated Learning. Available at: . "),e("a",{attrs:{href:"https://www.mdpi.com/1424-8220/17/4/905",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.mdpi.com/1424-8220/17/4/905"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[20]")]),t._v(" David Hughes Sharada Prasanna Mohanty and Marcel Salathe. Using Deep\nLearning for Image-Based Plant Disease Detection. Available at: . "),e("a",{attrs:{href:"https://www.frontiersin.org/articles/10.3389/fpls.2016.01419/full",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.frontiersin.org/articles/10.3389/fpls.2016.01419/full"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[21]")]),t._v(" Malusi Sibiya and Mbuyu Sumbwanyambe. A Computational Procedure for\nthe Recognition and Classification of Maize Leaf Diseases Out of Healthy\nLeaves Using Convolutional Neural Networks. Available at: . "),e("a",{attrs:{href:"https://www.mdpi.com/2624-7402/1/1/9",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.mdpi.com/2624-7402/1/1/9"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[22]")]),t._v(" Andras Anderla Dubravko Culibrk Srdjan Sladojevic, Marko Arsenovic and\nDarko Stefanovic. Deep Neural Networks Based Recognition of Plant Diseases\nby Leaf Image Classification. Available at: . "),e("a",{attrs:{href:"https://www.hindawi.com/journals/cin/2016/3289801/",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.hindawi.com/journals/cin/2016/3289801/"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[23]")]),t._v(" Richard N. Strange. and Peter R. Scott. Plant Disease: A Threat to Global\nFood Security. Available online at. "),e("a",{attrs:{href:"https://www.annualreviews.org/doi/full/10.1146/annurev.phyto.43.113004.133839",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.annualreviews.org/doi/full/10.1146/annurev.phyto.43.113004.133839"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[24]")]),t._v(" MUAMMER TURKOGLU and DAVUT HANBAY. Plant disease and\npest detection using deep learning-based features. Available at: . "),e("a",{attrs:{href:"https://journals.tubitak.gov.tr/elektrik/abstract.htm?id=24806",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://journals.tubitak.gov.tr/elektrik/abstract.htm?id=24806"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[25]")]),t._v(" Faina Khoroshevsky Alon Shpigler Aharon Bar Hillel Yotam Itzhaky,\nGuy Farjon. Leaf Counting: Multiple Scale Regression and Detection\nUsing Deep CNNs. Available at: . "),e("a",{attrs:{href:"https://www.plant-phenotyping.org/lw_resource/datapool/systemfiles/elements/files/63b7a231-949c-11e8-8a88-dead53a91d31/current/document/0031.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.plant-phenotyping.org/lw_resource/datapool/systemfiles/elements/files/63b7a231-949c-11e8-8a88-dead53a91d31/current/document/0031.pdf"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("[26]")]),t._v(" Guan Wang Yu Sun, Yuan Liu and Haiyan Zhang. Deep Learning for\nPlant Identification in Natural Environment. Available at: . "),e("a",{attrs:{href:"https://www.hindawi.com/journals/cin/2017/7361042/",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.hindawi.com/journals/cin/2017/7361042/"),e("OutboundLink")],1),t._v(".")])])}),[],!1,null,null,null);a.default=n.exports}}]);