<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Table of Content | Plantlet</title>
    <meta name="generator" content="VuePress 1.5.3">
    <link rel="icon" href="/plantlet-documentation/favicon.svg">
    <meta name="description" content="Detection of disease in plants using deep learning">
    <link rel="preload" href="/plantlet-documentation/assets/css/0.styles.b300be4e.css" as="style"><link rel="preload" href="/plantlet-documentation/assets/js/app.9fb1ed09.js" as="script"><link rel="preload" href="/plantlet-documentation/assets/js/2.13400713.js" as="script"><link rel="preload" href="/plantlet-documentation/assets/js/6.67a8e0b0.js" as="script"><link rel="prefetch" href="/plantlet-documentation/assets/js/3.37a74d58.js"><link rel="prefetch" href="/plantlet-documentation/assets/js/4.45a5b420.js"><link rel="prefetch" href="/plantlet-documentation/assets/js/5.a9b27031.js">
    <link rel="stylesheet" href="/plantlet-documentation/assets/css/0.styles.b300be4e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/plantlet-documentation/" class="home-link router-link-active"><!----> <span class="site-name">Plantlet</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/plantlet-documentation/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/plantlet-documentation/thesis/thesis/" class="nav-link">
  Thesis
</a></div><div class="nav-item"><a href="https://cutt.ly/FaHO7kM" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Download APK
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://github.com/muhammadosmanali/fyp-project" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Source Code
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/plantlet-documentation/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/plantlet-documentation/thesis/thesis/" class="nav-link">
  Thesis
</a></div><div class="nav-item"><a href="https://cutt.ly/FaHO7kM" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Download APK
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://github.com/muhammadosmanali/fyp-project" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Source Code
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/plantlet-documentation/thesis/" aria-current="page" class="active sidebar-link">Table of Content</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#abbreviations" class="sidebar-link">Abbreviations</a></li><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#abstract" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#chapter-1-introduction" class="sidebar-link">Chapter 1: Introduction</a></li><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#chapter-2-motivation-and-problem-statement" class="sidebar-link">Chapter 2: Motivation and Problem Statement</a></li><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#chapter-3-literature-review" class="sidebar-link">Chapter 3: Literature Review</a></li><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#chapter-4-materials-and-methods" class="sidebar-link">Chapter 4: Materials and Methods</a></li><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#chapter-5-implementation-details" class="sidebar-link">Chapter 5: Implementation Details</a></li><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#chapter-6-results" class="sidebar-link">Chapter 6: Results</a></li><li class="sidebar-sub-header"><a href="/plantlet-documentation/thesis/#references" class="sidebar-link">References</a></li></ul></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="table-of-content"><a href="#table-of-content" class="header-anchor">#</a> Table of Content</h1> <p></p><div class="table-of-contents"><ul><li><a href="#abbreviations">Abbreviations</a></li><li><a href="#abstract">Abstract</a></li><li><a href="#chapter-1-introduction">Chapter 1: Introduction</a></li><li><a href="#chapter-2-motivation-and-problem-statement">Chapter 2: Motivation and Problem Statement</a><ul><li><a href="#_2-1-motivation">2.1: Motivation</a></li><li><a href="#_2-2-problem-statement">2.2: Problem Statement</a></li></ul></li><li><a href="#chapter-3-literature-review">Chapter 3: Literature Review</a></li><li><a href="#chapter-4-materials-and-methods">Chapter 4: Materials and Methods</a><ul><li><a href="#_4-1-materials">4.1: Materials</a></li><li><a href="#_4-2-methods">4.2: Methods</a></li></ul></li><li><a href="#chapter-5-implementation-details">Chapter 5: Implementation Details</a><ul><li><a href="#_5-1-snippets-of-model-training-code">5.1: Snippets of Model Training Code</a></li><li><a href="#_5-2-snippets-of-app-user-interface">5.2: Snippets of App User Interface</a></li></ul></li><li><a href="#chapter-6-results">Chapter 6: Results</a></li><li><a href="#references">References</a></li></ul></div><p></p> <h2 id="abbreviations"><a href="#abbreviations" class="header-anchor">#</a> Abbreviations</h2> <p><strong>ML</strong> Machine Learning<br> <strong>DL</strong> Deep Learning<br> <strong>CNN</strong> Convolutional Neural Network<br> <strong>DCNN</strong> Deep Convolutional Neural Network<br> <strong>ILSVRC</strong> ImageNet Large Scale Visual Recognition Challenge<br> <strong>VGG</strong> Visual Geometry Group <strong>(</strong> Convolutional neural network architecture <strong>)</strong><br> <strong>COCO</strong> Common Objects in Context</p> <h2 id="abstract"><a href="#abstract" class="header-anchor">#</a> Abstract</h2> <p>With the increase in population, the needs have also been increased. As agriculture is a vital source for providing food, whereas crop ailments are major commination to agriculture, and their early detection remains strenuous over the globe due to insufficient technology, agricultural organizations are unable to reach farmers in time for necessary precautionary measures. As a result, farmers have to suffer from compromised lower crop yield. Many machine learning models were used to detect and identify diseases of plants but, after the advancement in Deep Learning, this field seems to have great potential concerning improved accuracy. The combination of advancements in computer vision and the global smartphone penetration made possible by deep learning to provide a mobile-phone-based system to diagnose diseases. Using a public data-set of <strong>87,900</strong> photos of healthy and diseased leaves, several models were trained to identify <strong>14</strong> crops and the presence or absence of <strong>26</strong> diseases, with the best performance reaching a <strong>98.58%</strong> on the retained dataset, which demonstrates the practicality of our perspective. Generally, the methodology of training deep convolutional neural networks on exceptionally huge and publicly accessible image data-sets presents a straightforward path towards a massive global diagnosis of mobile-phone-assisted crop disease.</p> <h2 id="chapter-1-introduction"><a href="#chapter-1-introduction" class="header-anchor">#</a> Chapter 1: Introduction</h2> <p>Earth is occupying more than 7 billion people. The number is increasing gradually.
With a persistent increase in population, it is understood that basic life necessities
are also increasing in which food is on top of the list. The increasing population
has also eaten up the land, and the area for agricultural land is shrinking up.
So we have to get an adequate amount of food from the land available. Food
sustainability, however, remains endanger by a variety of factors which includes
climate change [<a href="#references">5</a>], plant diseases [<a href="#references">23</a>] and others. When the plants are growing up,
they are attacked by diseases which clearly means a compromised lower crop yield.
Unfortunately, smallholder farmers have to face the disastrous consequences whose
income entirely depends upon crops. Yield. The major agricultural production
comes from smallholder farmers [<a href="#references">9</a>], and they have to suffer approximately 50%
yield loss due to climate change, pest attack, and diseases. Acknowledging these
problems, various attempts have been made to avert or lower the loss of crop yield.
To prevent the disease, it is crucial to detect the disease at an early stage. And
efficient disease management is a very crucial step in this regard. Agricultural
organizations have been working for disease detection at an early stage at local
clinics. During the past decades the world has totally turned into ”Global Village”
and because of that enormous data is available online including information on
disease diagnosis [<a href="#references">11</a>] and the leverage of which is internet penetration worldwide.
More recently, mobile phone technology incredibly has become famous due to the
proliferation of mobile-based tools in all parts of the world. All these factors
escort us to a point where disease detection is technically feasible and available at
an unparalleled scale. Unlike other countries, Pakistan lacks modern technology
due to which, we are unable to detect diseases in time and to reach farmers in
order to raise awareness about rehabilitation. The blue-collar approaches make
it very slow for policy-making organizations to gather data and draw results for
immediate movement. The intensity behind this research work is to dispense
an efficient system that can detect the disease straight-away whether a farmer
or agricultural organization uses it. We intended to develop mobile as well as
a web-based tool using deep learning for the detection of crop diseases. Deep
learning has proved its worth successfully in many different domains such as
end-to-end learning. Now we are going to demonstrate the technical feasibility of our
proposed approach by utilizing 87,900 images on healthy and infected leaves of
crop plants that are openly available on the online system PlantVillage [<a href="#references">2</a>].</p> <img src="/plantlet-documentation/thesis/ExampleDatasetImages.jpg" alt="Sample leaf images from the dataset"> <hr> <span style="text-align:center;display:block;"><strong>Figure 1.1:</strong> Sample leaf images from the dataset</span> <hr> <p>A DCNN includes the mapping between an input to an output. Deep learning
is probability-based means it never gives us the definite answer however it gives
us the probabilities. The term CNN itself stipulates that a mathematical function called convolution is used within the network. Convolution is a specific type
mathematical operation on two functions which produces a third function that
expresses how another changes one’s form. A CNN consists of two main input and
output layers, and multiple hidden layers. Typically the hidden layers consist of
a series of convolutionary layers. The activation function is commonly a Rectified
Linear Units (RELU). The purpose of RELU is to normalize the values after the
application of convolution to convert the values in a specific range. Additional
convolutions such as pooling, fully connected layers and normalization layers follow the RELU. Pooling is to choose the best or one thing out of the pool of things.
The nodes in neural networks are computational units which take weighted inputs
from the incoming edges and provide an outgoing edge with numerical output.
node enumerates an output value by adding a specific function to the previous
layer’s input values. In a neural network, model learning progresses through iterative changes to these weights and biases. DCNN is learned by changing network
parameters in such a way as to improve mapping during the training. For thepurpose of plant disease identification, we needed a large and verified data-set of
healthy and diseased images to train an accurate image classifier, but such dataset
did not exist until very recently, and even small dataset were not publicly available. In order to tackle this issue, the PlantVillage project began to collect a large
dataset of diseased and healthy crop plants and made them available publicly. We
announce here on the classification of 26 diseases (presence or absence) in 14 crop
species using 87,900 with deep learning (DL).</p> <h2 id="chapter-2-motivation-and-problem-statement"><a href="#chapter-2-motivation-and-problem-statement" class="header-anchor">#</a> Chapter 2: Motivation and Problem Statement</h2> <h3 id="_2-1-motivation"><a href="#_2-1-motivation" class="header-anchor">#</a> 2.1: Motivation</h3> <p>With the increase in population, the needs have also been increased. As agriculture is the vital source for providing food but, unfortunately, because of lack the
modern technology, the agricultural organization are unable to reach farmers in
time for necessary precautionary measures. As a results farmers have to suffer
from compromised lower crop yield. We are aimed to work for a solution that can
prevent farmers from suffering a loss to some extent.</p> <h3 id="_2-2-problem-statement"><a href="#_2-2-problem-statement" class="header-anchor">#</a> 2.2: Problem Statement</h3> <p>Plant disease affects not only the production of human food but also natural
systems. The majority of smallholder farmers do not have access to the resources
that can identify the diseases accurately and timely. Due to the lack of timely and
accurate agricultural information, they have to suffer from lower crop yield.</p> <h2 id="chapter-3-literature-review"><a href="#chapter-3-literature-review" class="header-anchor">#</a> Chapter 3: Literature Review</h2> <p>The Deep Learning (DL) is subcategory of ML. This field is still evolving. Machine
Learning has made enormous evolution in the past few years. Many advances were
found in the first phase, such as handwritten text recognition, back-propagation
and resolving training problems. In the second phase was to develop algorithms
for health-sectors, text-recognition, earthquake-predictions, marketing, finance,
image-recognition, and object detection. In 2012 a deep convolutionary neural
network accomplished a top-5 error of 15.3% when classifying images into 1000
possible categories [<a href="#references">1</a>]. In the next three years, numerous advances in convolutionary neural networks reduced the error rate to 3.57. With the passage of time as
Deep Learning architectures started to evolve, researchers applied them to classification, segmentation, object detection, video processing, natural language processing, image recognition, and speech recognition. Different agriculture application
has also been developed using these architectures. For example, Leaf counting was
performed using Deep CNN with average accuracy of 95% [<a href="#references">25</a>]. Leaf classification
was performed using deep convolutional neural network classifier among 32 different species with average accuracy of 97.3% [<a href="#references">7</a>]. Fruit counting was performed by
using simulated deep convolutional neural network with an average accuracy of
91% [<a href="#references">19</a>]. Classification of land cover and crop type was performed by using deep
learning classifier with accuracy of crop type identification of 95% [<a href="#references">18</a>]. Identification of plants was performed by using Deep convolutional neural network [<a href="#references">10</a>], [<a href="#references">16</a>].
In [<a href="#references">26</a>] identification of plants was performed among 100 different species utilizing
10,000 images using deep learning with an accuracy of 91.78%. In addition deep
learning techniques are also used in for crucial tasks such as crop plant disease
identification and classification which is main topic of this thesis. For example,
in [<a href="#references">20</a>] plant disease detection was performed using deep convolutional neural network classifier. To sum up, used DL Architectures are shown in the table along with the selected plants and their results.</p> <table><thead><tr><th>Deep Learning Techniques</th> <th>Dataset</th> <th>Used Plants</th> <th>Accuracy</th> <th>Reference</th></tr></thead> <tbody><tr><td>Convolutional Neural Network</td> <td>Plant Village </td> <td>Maize</td> <td>92.85%</td> <td>[<a href="#references">21</a>]</td></tr> <tr><td>LeNet</td> <td>Plant Village </td> <td>Banana</td> <td>98.54%</td> <td>[<a href="#references">4</a>]</td></tr> <tr><td>Alex-Net, VGG16, VGG-19, Squeeze-Net, Goog-LeNet, Inception-v-3, Inception-ResNet-v-2, ResNet-50, Resnet-101 </td> <td>Real Field Dataset</td> <td>Apricot, Walnut, Peach, Cherry </td> <td>97.14%</td> <td>[<a href="#references">24</a>]</td></tr> <tr><td>Inception-v-3 </td> <td>Experimental Field Dataset </td> <td>Cassava </td> <td>93%</td> <td>[<a href="#references">3</a>]</td></tr> <tr><td>Convolutional Neural Network</td> <td>Images Taken From The Research Center </td> <td>Cucumber </td> <td>82.3%</td> <td>[<a href="#references">8</a>]</td></tr> <tr><td>Super Resolution Convolutional Neural Network (SCRNN)</td> <td>Plant Village </td> <td>Tomato </td> <td>90%</td> <td>[<a href="#references">15</a>]</td></tr> <tr><td>Caffe-Net</td> <td>Downloaded From The Internet </td> <td>Pear, cherry, peach, apple, grapevine </td> <td>96.3%</td> <td>[<a href="#references">22</a>]</td></tr> <tr><td>Resnet-50, Inception-V-2, Mobile-Net-V-1 </td> <td>Real Environment </td> <td>Banana </td> <td>99% of ResNet-50</td> <td>[<a href="#references">17</a>]</td></tr>1
        <tr><td>Mobile-Net, Modified-Mobile-Net, Reduced-Mobile-Net</td> <td>Plant Village dataset</td> <td>24 Types Of Plants</td> <td>98.34% of reduced MobileNet </td> <td>[<a href="#references">14</a>]</td></tr></tbody></table> <hr> <span style="text-align:center;display:block;"><strong>Table 3.1:</strong> DL Architectures along with selected plant species and results</span> <hr> <p>From this table, we can conclude that while some Deep Learning Architectures/-
Models have been developed for the identification of the diseases of plants but this
is still a prolific research field and should lead to improvements for better plant
disease identification. So we needed a large and verified data set of healthy and
diseased images to train an accurate image classifier for plant disease diagnosis,
but such a dataset did not exist until very recently. Plant Village collected a huge
dataset of 87,900 images of plant health to enable the development of smartphone
assisted disease diagnosis and made it publicly and freely available [<a href="#references">12</a>].</p> <h2 id="chapter-4-materials-and-methods"><a href="#chapter-4-materials-and-methods" class="header-anchor">#</a> Chapter 4: Materials and Methods</h2> <h3 id="_4-1-materials"><a href="#_4-1-materials" class="header-anchor">#</a> 4.1: Materials</h3> <p>Our dataset is consists of 87,900 pictures of healthy and diseased plant leaves. All
the images in the dataset were taken at experimental research stations affiliated
with Land Grant Universities in the USA (Penn State, Florida State, Cornell, and
others) [<a href="#references">12</a>]. These images consist of 26 major crop diseases (4 bacterial diseases,
17 Fungal diseases, 2 viral diseases, 2 mold diseases, and 1 disease caused by a
mite). These images also have 12 healthy leaves of 12 crop species that are not
visibly affected by a disease. Table 4.1 Summarizes the dataset.</p> <table><thead><tr><th>Crop</th> <th>Classes</th></tr></thead> <tbody><tr><td>Apple</td> <td>Apple-Healthy, Apple-Cedar-Rust, Apple-Black-Rot, Apple-Scab</td></tr> <tr><td>Raspberry</td> <td>Raspberry-Healthy</td></tr> <tr><td>Soybean</td> <td>Soybean-Healthy</td></tr> <tr><td>Squash</td> <td>Squash-Powdery-Mildew</td></tr> <tr><td>Strawberry</td> <td>Strawberry-Leaf-Scorch, Strawberry-Healthy</td></tr> <tr><td>Tomato</td> <td>
        Tomato-Healthy, Tomato-Late-Blight, TomatoBacterial-Spot,
        Tomato-Early-Blight, TomatoLeaf-Mold, Tomato-Septoria-Leaf-Spot,
        TomatoMosaic-Virus, Tomato-Two-Spotted-Spider-Mite, Tomato-Target-Spot,
        Tomato-Yellow-Leaf-CurlVirus
      </td></tr> <tr><td>Potato</td> <td>Potato-Early-Blight, Potato-Late-Blight, PotatoHealthy</td></tr> <tr><td>Blueberry</td> <td>Blueberry-Healthy</td></tr> <tr><td>Cherry</td> <td>Cherry-Healthy, Cherry-Powdery-Mildew</td></tr> <tr><td>Corn</td> <td>
        Corn-Healthy, Corn-Common-Rust, Corn-Gray-Leaf Spot,
        Corn-Northern-Leaf-Blight
      </td></tr> <tr><td></td> <td></td></tr> <tr><td>Grape</td> <td>
        Grape-Healthy, Grape-Leaf-Blight, Grape-BlackRot,
        Grape-Black-Measles(Esca)
      </td></tr> <tr><td>Orange</td> <td>Orange-Huanglongbing (Citrus-Greening)</td></tr> <tr><td>Peach</td> <td>Peach-Healthy, Peach-Bacterial-Spot</td></tr> <tr><td>Bell-Pepper</td> <td>Bell-Pepper-Bacterial-Spot, Bell-Pepper-Healthy</td></tr></tbody></table> <hr> <span style="text-align:center;display:block;"><strong>Table 4.1:</strong> List of Crops with their respective classes in the Dataset.</span> <hr> <h3 id="_4-2-methods"><a href="#_4-2-methods" class="header-anchor">#</a> 4.2: Methods</h3> <p>Since these diseases can severely affect plants, we landed up with four different
models for 38 different classes (Table 4.1) to achieve maximum accuracy. Blueprint
of our proposed system for disease detection is showed in the figure 4.1.</p> <p>To train the model, we focused on 4 different architectures, namely VGG16,
Resnet152, MobileNet, and MobileNetV2.</p> <p>VGG16 was proposed by Andrew Zisserman and Karen Simonyan of the Visual
Geometry Community Lab at Oxford University. This model secured 1st and 2nd
place in ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014.
The VGG16 network is trained on an ImageNet dataset which has 14 million
images and 1000 classes, and this model achieves 92.7% top-5 test accuracy.</p> <p>Resnet was proposed by Microsoft Research Asia [<a href="#references">13</a>] and won the ImageNet
Large Scale Visual Recognition Challenge (ILSVRC) and MS-COCO competition in 2015. ResNet architecture has several variants all of them have the similar
concept but have different number of layers, for example ResNet-18, ResNet-50,
ResNet-101 and so on. The name Resnet followed by numbers simply insinuate
the Resnet architecture witch has number of layers of neural network. The main
idea manoeuvred in these models, residual network connections, is found to greatly
enhance gradient flow, thus enabling the training of tens or even hundreds of layers
of much deeper models.</p> <img src="/plantlet-documentation/thesis/fyp_model_image.jpeg" alt="Blueprint of our proposed system for disease detection"> <hr> <span style="text-align:center;display:block;"><strong>Figure 4.1:</strong> Blueprint of our proposed system for disease detection</span> <hr> <p>MobileNet is a model built primarily from depthwise separable convolutions to
create lightweight deep convolutional neural networks and provides an efficient
model for smartphone and integrated vision applications [<a href="#references">6</a>].</p> <p>To train these models we used Google Colaboratory framework. Google Colab
is a free cloud service created by Google, in which the person who has a Gmail
account can write, execute codes for Machine learning as well as for Deep learning.
In Google Colab different runtime environments and Various versions of python
are available. It can also download large datasets to google drive at higher speed
directly from the servers. With Google Colab, you can also mount your drive and it
can fetch the appropriate file after the authentication. The most important feature
of Google Colab that distinguishes it from other free cloud services is it provides
GPU and it is totally free. To summarize, we have 4 experimental configurations
in total, which are described in the next section</p> <h4 id="_4-2-1-experiment-1"><a href="#_4-2-1-experiment-1" class="header-anchor">#</a> 4.2.1: Experiment # 1</h4> <p>In this experiment, we used a python deep learning library called PyTorch, and
the model we used in this experiment is Resnet152. We used transfer learning to
train the model because it always yields better results. This experiment runs for
a total of 50 epochs and achieves testing accuracy of 98.5%.</p> <img src="/plantlet-documentation/thesis/fyp-model-accuracy-pytorch.png" alt="Model Accuracy"> <hr> <span style="text-align:center;display:block;"><strong>Figure 4.2:</strong> Model Accuracy</span> <hr> <img src="/plantlet-documentation/thesis/fyp-model-loss-pytorch.png" alt="Model Loss"> <hr> <span style="text-align:center;display:block;"><strong>Figure 4.3:</strong> Model Loss</span> <hr> <h4 id="_4-2-2-experiment-2"><a href="#_4-2-2-experiment-2" class="header-anchor">#</a> 4.2.2: Experiment # 2</h4> <p>In this experiment, we used a python deep learning library called TensorFlow, and
we created a custom model. This experiment runs for a total of 80 epochs and
achieves an accuracy of 94.72%.</p> <img src="/plantlet-documentation/thesis/precision-recall-accuracy-f1-score-custome-model.png" alt="Accuracy of the classifier for all classess"> <hr> <span style="text-align:center;display:block;"><strong>Figure 4.4:</strong> Accuracy of the classifier for all classes</span> <hr> <img src="/plantlet-documentation/thesis/classifier-graph.png" alt="Graph of all Classes and their Identifications"> <hr> <span style="text-align:center;display:block;"><strong>Figure 4.4:</strong> Graph of all Classes and their Identifications</span> <hr> <h2 id="chapter-5-implementation-details"><a href="#chapter-5-implementation-details" class="header-anchor">#</a> Chapter 5: Implementation Details</h2> <p>The very first was to gather the data and train the model to get the results. After
the collection of the dataset which is consisted of 87,900 different plant images, we
divided the dataset into train-test splits of 25-75, 50-50, and 25-75. We made these
splits to get a sense of how our proposed approach will perform on the unseen data
and also keep track of if our proposed approach is underfitting or overfitting. After
training the model with the best performance reaching a 98.58% on a held-out test
set we developed an API using Flask for this model to interact with our mobile
application. After the development of the API, we developed a mobile application
to interact with the API which interacts with the model to give the final results.</p> <h3 id="_5-1-snippets-of-model-training-code"><a href="#_5-1-snippets-of-model-training-code" class="header-anchor">#</a> 5.1: Snippets of Model Training Code</h3> <p>Here are some snippets of our code to train the model.</p> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> time
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token punctuation">,</span> optim
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms<span class="token punctuation">,</span> models
<span class="token keyword">import</span> torchvision
<span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> lr_scheduler
</code></pre></div><hr> <span style="text-align:center;display:block;"><strong>Figure 5.1:</strong> Libraries used in the model training</span> <hr> <div class="language-py extra-class"><pre class="language-py"><code>data_dir <span class="token operator">=</span> <span class="token string">'/content/DataSet'</span>
train_dir <span class="token operator">=</span> data_dir <span class="token operator">+</span> <span class="token string">'/train'</span>
valid_dir <span class="token operator">=</span> data_dir <span class="token operator">+</span> <span class="token string">'/val'</span>
nThreads <span class="token operator">=</span> <span class="token number">4</span>
batch_size <span class="token operator">=</span> <span class="token number">32</span>
</code></pre></div><hr> <span style="text-align:center;display:block;"><strong>Figure 5.2:</strong> Load the dataset</span> <hr> <div class="language-py extra-class"><pre class="language-py"><code>data_transforms <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">'train'</span><span class="token punctuation">:</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
        transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>RandomResizedCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">'val'</span><span class="token punctuation">:</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
        transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

data_dir <span class="token operator">=</span> <span class="token string">'/content/DataSet'</span>
image_datasets <span class="token operator">=</span> <span class="token punctuation">{</span>x<span class="token punctuation">:</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                          data_transforms<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span>
                  <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>

dataloaders <span class="token operator">=</span> <span class="token punctuation">{</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>image_datasets<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>
                                             shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>
              <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>

dataset_sizes <span class="token operator">=</span> <span class="token punctuation">{</span>x<span class="token punctuation">:</span> <span class="token builtin">len</span><span class="token punctuation">(</span>image_datasets<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>

class_names <span class="token operator">=</span> image_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>classes
</code></pre></div><hr> <span style="text-align:center;display:block;"><strong>Figure 5.3:</strong> Applying Data Augmentation</span> <hr> <div class="language-py extra-class"><pre class="language-py"><code>model <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet152<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>

<span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict

classifier <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>
                          <span class="token punctuation">(</span><span class="token string">'fc1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                          <span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                          <span class="token punctuation">(</span><span class="token string">'fc2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">39</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                          <span class="token punctuation">(</span><span class="token string">'output'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                          <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>fc <span class="token operator">=</span> classifier
</code></pre></div><hr> <span style="text-align:center;display:block;"><strong>Figure 5.4:</strong> Building the Classifier</span> <hr> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">def</span> <span class="token function">train_model</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> scheduler<span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    since <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    best_model_wts <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    best_acc <span class="token operator">=</span> <span class="token number">0.0</span>

    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch {}/{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> num_epochs<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'-'</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> phase <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>
                scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
                model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>  
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  

            running_loss <span class="token operator">=</span> <span class="token number">0.0</span>
            running_corrects <span class="token operator">=</span> <span class="token number">0</span>

            <span class="token keyword">for</span> inputs<span class="token punctuation">,</span> labels <span class="token keyword">in</span> dataloaders<span class="token punctuation">[</span>phase<span class="token punctuation">]</span><span class="token punctuation">:</span>
                inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

                <span class="token keyword">with</span> torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span>phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
                    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
                    _<span class="token punctuation">,</span> preds <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

                    <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>
                        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
                        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

                running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> inputs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
                running_corrects <span class="token operator">+=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>preds <span class="token operator">==</span> labels<span class="token punctuation">.</span>data<span class="token punctuation">)</span>

            epoch_loss <span class="token operator">=</span> running_loss <span class="token operator">/</span> dataset_sizes<span class="token punctuation">[</span>phase<span class="token punctuation">]</span>
            epoch_acc <span class="token operator">=</span> running_corrects<span class="token punctuation">.</span>double<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> dataset_sizes<span class="token punctuation">[</span>phase<span class="token punctuation">]</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{} Loss: {:.4f} Acc: {:.4f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
                phase<span class="token punctuation">,</span> epoch_loss<span class="token punctuation">,</span> epoch_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'val'</span> <span class="token keyword">and</span> epoch_acc <span class="token operator">&gt;</span> best_acc<span class="token punctuation">:</span>
                best_acc <span class="token operator">=</span> epoch_acc
                best_model_wts <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    time_elapsed <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> since
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Training complete in {:.0f}m {:.0f}s'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
        time_elapsed <span class="token operator">//</span> <span class="token number">60</span><span class="token punctuation">,</span> time_elapsed <span class="token operator">%</span> <span class="token number">60</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Best valid accuracy: {:4f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>best_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>best_model_wts<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
</code></pre></div><hr> <span style="text-align:center;display:block;"><strong>Figure 5.5:</strong> Function to train the model</span> <hr> <div class="language-py extra-class"><pre class="language-py"><code>num_epochs <span class="token operator">=</span> <span class="token number">80</span>

criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
exp_lr_scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
model_ft <span class="token operator">=</span> train_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> exp_lr_scheduler<span class="token punctuation">,</span> num_epochs<span class="token operator">=</span>num_epochs<span class="token punctuation">)</span>
</code></pre></div><hr> <span style="text-align:center;display:block;"><strong>Figure 5.6:</strong> : Start Training</span> <hr> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> dataloaders<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
  model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  accuracy <span class="token operator">=</span> <span class="token number">0</span>
  model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    
  <span class="token keyword">for</span> images<span class="token punctuation">,</span> labels <span class="token keyword">in</span> dataloaders<span class="token punctuation">[</span><span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    images <span class="token operator">=</span> Variable<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
    labels <span class="token operator">=</span> Variable<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>
    images<span class="token punctuation">,</span> labels <span class="token operator">=</span> images<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    output <span class="token operator">=</span> model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
    ps <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
    equality <span class="token operator">=</span> <span class="token punctuation">(</span>labels<span class="token punctuation">.</span>data <span class="token operator">==</span> ps<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    accuracy <span class="token operator">+=</span> equality<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Testing Accuracy: {:.3f}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>accuracy<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataloaders<span class="token punctuation">[</span><span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

test<span class="token punctuation">(</span>model<span class="token punctuation">,</span> dataloaders<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
</code></pre></div><hr> <span style="text-align:center;display:block;"><strong>Figure 5.7:</strong> Model validation after Training</span> <hr> <div class="language-py extra-class"><pre class="language-py"><code>model<span class="token punctuation">.</span>class_to_idx <span class="token operator">=</span> dataloaders<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>class_to_idx
model<span class="token punctuation">.</span>epochs <span class="token operator">=</span> num_epochs
checkpoint <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'input_size'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                 <span class="token string">'batch_size'</span><span class="token punctuation">:</span> dataloaders<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span>
                  <span class="token string">'output_size'</span><span class="token punctuation">:</span> <span class="token number">39</span><span class="token punctuation">,</span>
                  <span class="token string">'state_dict'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  <span class="token string">'data_transforms'</span><span class="token punctuation">:</span> data_transforms<span class="token punctuation">,</span>
                  <span class="token string">'optimizer_dict'</span><span class="token punctuation">:</span>optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  <span class="token string">'class_to_idx'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>class_to_idx<span class="token punctuation">,</span>
                  <span class="token string">'epoch'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>epochs<span class="token punctuation">}</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> <span class="token string">'plants9615_checkpoint.pth'</span><span class="token punctuation">)</span>
</code></pre></div><hr> <span style="text-align:center;display:block;"><strong>Figure 5.8:</strong> Saving the model for Further use</span> <hr> <h3 id="_5-2-snippets-of-app-user-interface"><a href="#_5-2-snippets-of-app-user-interface" class="header-anchor">#</a> 5.2: Snippets of App User Interface</h3> <p>User interface for the mobile application is shown in the upcoming pictures.</p> <img src="/plantlet-documentation/thesis/2_sign_up_page.png" alt="Sign Up Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.9:</strong> Sign Up Screen</span> <hr> <img src="/plantlet-documentation/thesis/1_login_page.png" alt="Login Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.10:</strong> Login Screen</span> <hr> <img src="/plantlet-documentation/thesis/3_home_page.png" alt="Home Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.11:</strong> Home Screen</span> <hr> <img src="/plantlet-documentation/thesis/4_community_page.png" alt="Community Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.12:</strong> Community Screen</span> <hr> <img src="/plantlet-documentation/thesis/5_user_page.png" alt="User Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.13:</strong>User Screen</span> <hr> <img src="/plantlet-documentation/thesis/6_add_post_page.png" alt="Add Post Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.14:</strong>Add Post Screen</span> <hr> <img src="/plantlet-documentation/thesis/7_layout_nav.png" alt="Logout Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.15:</strong> Logout Screen</span> <hr> <img src="/plantlet-documentation/thesis/8_prediction1.png" alt="Prediction # 1 Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.16:</strong> Prediction # 1 Screen</span> <hr> <img src="/plantlet-documentation/thesis/9_prediction2.png" alt="Prediction # 2 Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.17:</strong> Prediction # 2 Screen</span> <hr> <img src="/plantlet-documentation/thesis/10_prediction3.png" alt="Prediction # 3 Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.18:</strong> Prediction # 3 Screen</span> <hr> <img src="/plantlet-documentation/thesis/11_prediction4.png" alt="Prediction # 4 Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.19:</strong> Prediction # 4 Screen</span> <hr> <img src="/plantlet-documentation/thesis/12_prediction5.png" alt="Prediction # 5 Screen" width="300" height="568"> <hr> <span style="text-align:center;display:block;"><strong>Figure 5.20:</strong> Prediction # 5 Screen</span> <hr> <h2 id="chapter-6-results"><a href="#chapter-6-results" class="header-anchor">#</a> Chapter 6: Results</h2> <p>All results were observed on the premise that the trained model has to identify
the crop genus and the state of the infection. After training model with different
configurations the overall accuracy we procured on the collected dataset varied
from 75% to 98.58% which showing us the strong promise of a DL approach to
similar problems. Mean accuracy’s across our experiments is shown in the table
given below.</p> <table><thead><tr><th>Experiment</th> <th>Category</th> <th>Dataset Splits</th> <th>Model Used</th> <th>Epochs</th> <th>Mean Accuracy</th></tr></thead> <tbody><tr><td>1</td> <td>Transfer Learning</td> <td>70% Train, 30% Validation, Testing</td> <td>VGG16</td> <td>30</td> <td>94.72%</td></tr> <tr><td>2</td> <td>Transfer Learning</td> <td>70% Train, 30% Validation, Testing</td> <td>Resnet152</td> <td>30</td> <td>96.43%</td></tr> <tr><td>3</td> <td>Transfer Learning</td> <td>70% Train, 30% Validation, Testing</td> <td>Resnet152</td> <td>50</td> <td>98.5849%</td></tr> <tr><td>4</td> <td>Custom Model</td> <td>70% Train, 30% Validation, Testing</td> <td>Created Custom Model</td> <td>50</td> <td>96%</td></tr></tbody></table> <hr> <span style="text-align:center;display:block;"><strong>Table 6.1:</strong> Mean accuracy’s across all experiments</span> <hr> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <p><strong>[1]</strong>  Ilya Sutskever Alex Krizhevsky and Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. Online Available at: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener noreferrer">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><strong>[2]</strong>  Abdallah Ali. PlantVillage Dataset Dataset of diseased plant leaf images and corresponding labels. Available at: <a href="https://www.kaggle.com/xabdallahali/plantvillage-dataset" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/xabdallahali/plantvillage-dataset<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><strong>[3]</strong> Peter McCloskey Babuali Ahmed Ally James Legg Amanda Ramcharan,
Kelsee Baranowski and David P. Hughes. Deep Learning for Image-Based
Cassava Disease Detection. Available at: . <a href="https://www.frontiersin.org/articles/10.3389/fpls.2017.01852/full?report=reader" target="_blank" rel="noopener noreferrer">https://www.frontiersin.org/articles/10.3389/fpls.2017.01852/full?report=reader<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[4]</strong> Bouaziz Bassem Amara Jihen and Algergawy Alsayed. A Deep Learningbased Approach for Banana Leaf Diseases Classification. Available at: .
<a href="https://dl.gi.de/handle/20.500.12116/944" target="_blank" rel="noopener noreferrer">https://dl.gi.de/handle/20.500.12116/944<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[5]</strong> Maria Val Martin Amos P. K. Tai and Colette L. Heald. Threat to future
global food security from climate change and ozone air pollution. Available
online at. <a href="https://www.nature.com/articles/nclimate2317" target="_blank" rel="noopener noreferrer">https://www.nature.com/articles/nclimate2317<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[6]</strong> Bo Chen Dmitry Kalenichenko Weijun Wang Tobias Weyand Marco Andreetto Andrew G. Howard, Menglong Zhu and Hartwig Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. Available at: . <a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1704.04861.pdf<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[7]</strong> Feras Dayoub Niko Sunderhauf David Hall, Chris McCool and Ben Upcroft. Evaluation of Features for Leaf Classification in Challenging Conditions. Available at: . <a href="https://ieeexplore.ieee.org/abstract/document/7045965" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/abstract/document/7045965<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[8]</strong> Hiroyuki Uga Satoshi Kagiwada Erika Fujita, Yusuke Kawasaki and Hitoshi Iyatomi. Basic Investigation on a Robust and Practical Plant Diagnostic System. Available at: . <a href="https://ieeexplore.ieee.org/abstract/document/7838282" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/abstract/document/7838282<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[9]</strong> International Fund for Agricultural Development (IFAD) andUnited Nations Environment Programme (UNEP). Smallholders, food security, and the environment. Available online at. <a href="https://www.ifad.org/documents/38714170/39135645/smallholders_report.pdf/133e8903-0204-4e7d-a780-bca847933f2e" target="_blank" rel="noopener noreferrer">https://www.ifad.org/documents/38714170/39135645/smallholders_report.pdf/133e8903-0204-4e7d-a780-bca847933f2e<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[10]</strong> Monica G.Larese Guillermo L.Grinblat, Lucas C.Uzal and Pablo M.Granitto. Deep learning for plant identification using vein morphological patterns. Available at: . <a href="https://www.sciencedirect.com/science/article/pii/S0168169916304665" target="_blank" rel="noopener noreferrer">https://www.sciencedirect.com/science/article/pii/S0168169916304665<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[11]</strong> David. P. Hughes and Marcel Salathe. An open access repository of images on plant health to enable the development of mobile disease diagnostics. Available online at. <a href="https://arxiv.org/abs/1511.08060" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1511.08060<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><strong>[12]</strong> David P. Hughes and Marcel Salathe. An open access repository of images on plant health to enable the development of mobile disease diagnostics. Available at: . <a href="https://arxiv.org/ftp/arxiv/papers/1511/1511.08060.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/ftp/arxiv/papers/1511/1511.08060.pdf<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><strong>[13]</strong> Shaoqing Ren Jian Sun (Microsoft Research) Kaiming He, Xiangyu Zhang. Deep Residual Learning for Image Recognition. Available at: .<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1512.03385.pdf<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[14]</strong> Mingyang Wu Kamal KC, Zhendong Yin and Zhilo Wu. Depthwise separable convolution architectures for plant disease classification.Available at: . <a href="https://www.sciencedirect.com/science/article/pii/S0168169918318696" target="_blank" rel="noopener noreferrer">https://www.sciencedirect.com/science/article/pii/S0168169918318696<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[15]</strong> Takashi Togami Kyosuke Yamamoto and Norio Yamaguchi. Super-Resolution of Plant Disease Images for the Acceleration of Image-based Phenotyping and Vigor Diagnosis in Agriculture. Available at: . <a href="https://www.mdpi.com/1424-8220/17/11/2557" target="_blank" rel="noopener noreferrer">https://www.mdpi.com/1424-8220/17/11/2557<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[16]</strong> Sue Han Lee and Chee Seng Chan. Deep-plant: Plant identification with convolutional neural networks. Available at: .
<a href="https://ieeexplore.ieee.org/abstract/document/7350839" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/abstract/document/7350839<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[17]</strong> Henry Ruiz Nancy Safari Sivalingam Elayabalan Walter Ocimati Michael
Gomez Selvaraj, Alejandro Vergara and Guy Blomme. AI-powered banana
diseases and pest detection. Available at: . <a href="https://link.springer.com/article/10.1186/s13007-019-0475-z" target="_blank" rel="noopener noreferrer">https://link.springer.com/article/10.1186/s13007-019-0475-z<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[18]</strong> S. Skakun Nataliia Kussul, Mykola Lavreniuk and Andrii Shelestov. Deep
Learning Classification of Land Cover and Crop Types Using Remote Sensing
Data. Available at: . <a href="https://ieeexplore.ieee.org/document/7891032" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/document/7891032<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[19]</strong> Maryam Rahnemoonfar and Clay Sheppard. Deep Count: Fruit Counting
Based on Deep Simulated Learning. Available at: . <a href="https://www.mdpi.com/1424-8220/17/4/905" target="_blank" rel="noopener noreferrer">https://www.mdpi.com/1424-8220/17/4/905<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[20]</strong> David Hughes Sharada Prasanna Mohanty and Marcel Salathe. Using Deep
Learning for Image-Based Plant Disease Detection. Available at: . <a href="https://www.frontiersin.org/articles/10.3389/fpls.2016.01419/full" target="_blank" rel="noopener noreferrer">https://www.frontiersin.org/articles/10.3389/fpls.2016.01419/full<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[21]</strong> Malusi Sibiya and Mbuyu Sumbwanyambe. A Computational Procedure for
the Recognition and Classification of Maize Leaf Diseases Out of Healthy
Leaves Using Convolutional Neural Networks. Available at: . <a href="https://www.mdpi.com/2624-7402/1/1/9" target="_blank" rel="noopener noreferrer">https://www.mdpi.com/2624-7402/1/1/9<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[22]</strong> Andras Anderla Dubravko Culibrk Srdjan Sladojevic, Marko Arsenovic and
Darko Stefanovic. Deep Neural Networks Based Recognition of Plant Diseases
by Leaf Image Classification. Available at: . <a href="https://www.hindawi.com/journals/cin/2016/3289801/" target="_blank" rel="noopener noreferrer">https://www.hindawi.com/journals/cin/2016/3289801/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[23]</strong> Richard N. Strange. and Peter R. Scott. Plant Disease: A Threat to Global
Food Security. Available online at. <a href="https://www.annualreviews.org/doi/full/10.1146/annurev.phyto.43.113004.133839" target="_blank" rel="noopener noreferrer">https://www.annualreviews.org/doi/full/10.1146/annurev.phyto.43.113004.133839<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[24]</strong> MUAMMER TURKOGLU and DAVUT HANBAY. Plant disease and
pest detection using deep learning-based features. Available at: . <a href="https://journals.tubitak.gov.tr/elektrik/abstract.htm?id=24806" target="_blank" rel="noopener noreferrer">https://journals.tubitak.gov.tr/elektrik/abstract.htm?id=24806<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[25]</strong> Faina Khoroshevsky Alon Shpigler Aharon Bar Hillel Yotam Itzhaky,
Guy Farjon. Leaf Counting: Multiple Scale Regression and Detection
Using Deep CNNs. Available at: . <a href="https://www.plant-phenotyping.org/lw_resource/datapool/systemfiles/elements/files/63b7a231-949c-11e8-8a88-dead53a91d31/current/document/0031.pdf" target="_blank" rel="noopener noreferrer">https://www.plant-phenotyping.org/lw_resource/datapool/systemfiles/elements/files/63b7a231-949c-11e8-8a88-dead53a91d31/current/document/0031.pdf<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>[26]</strong> Guan Wang Yu Sun, Yuan Liu and Haiyan Zhang. Deep Learning for
Plant Identification in Natural Environment. Available at: . <a href="https://www.hindawi.com/journals/cin/2017/7361042/" target="_blank" rel="noopener noreferrer">https://www.hindawi.com/journals/cin/2017/7361042/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/plantlet-documentation/assets/js/app.9fb1ed09.js" defer></script><script src="/plantlet-documentation/assets/js/2.13400713.js" defer></script><script src="/plantlet-documentation/assets/js/6.67a8e0b0.js" defer></script>
  </body>
</html>
